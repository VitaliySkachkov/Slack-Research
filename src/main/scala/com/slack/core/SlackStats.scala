package com.slack.core

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql._
import org.apache.spark.sql.types.{StructField, _}
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

import annotation.tailrec
import org.apache.spark.HashPartitioner

import scala.reflect.ClassTag
import java.sql.DriverManager
import java.text.{FieldPosition, ParsePosition}
import java.util.Date

import org.joda.time.DateTime

import scala.io.Source

/**
  * Сообщение из публичного канала Slack
  * 1. channel
  * 2. channel_type
  * 3. msgType
  * 4. user
  * 5. event_time
  * 6. text
  */

case class ChannelMessage(channel:String, channel_type:String, msgType:String, user:String, event_time:String, text:Option[String]) extends Serializable
  {
    def mentionsWord(word: String): Boolean = text.get.toLowerCase.split(' ').contains(word)
  }
/** A raw stackoverflow posting, either a question or an answer */


object SlackStats extends SlackStats
  {

  @transient lazy val conf: SparkConf = new SparkConf().setAppName("VSkachkov_SlackStats").setMaster("local")
  @transient lazy val sc: SparkContext = SparkContext.getOrCreate(conf)
  @transient lazy val spark: SparkSession = SparkSession.builder
                                                        .appName("Vitaliy Slack research")  // optional and will be autogenerated if not specified
                                                        .master("local")
                                                        .getOrCreate()

    def main(args: Array[String]) {

      val filePath = args(0)

    /** [1]: basic wordcount computation */

    //  val SlackRdd: RDD[String] = sc.textFile(s"$filePath").cache()
    //  val raw      = rawMessage(SlackRdd)
    //  val calc_rdd = msgCount(raw) // wordsCount(raw, 10)
    //  calc_rdd.foreach(x => println("Date: " +  x._1._2 + "; Channel: " +  x._1._1 + "; Messages send: " + x._2.toString))
    //  val wordCount_val: Int = wordCount("for", raw)
    //  println("Word written:" + wordCount_val.toString)


    /** [2]: word distribution calculation */
      decomposeMsg(spark, filePath)

    /** [3]: write RDD to MySQ */
    //  dbWrite_RDD(calc_rdd)

    /** [4]: DataSet Examples ( 1 - SparkSession, 2 - only SparkContext, 3 - JSON format ) */
    //   ChannelMessageDF (spark, filePath)
    //   ChannelMessageDF_2 (sc, SlackRdd)
    //  ChannelMessageDF_JSON (spark, filePath)

      sc.stop()
      spark.stop()
  }
}

class SlackStats extends Serializable {

  def rawMessage(lines: RDD[String]): RDD[ChannelMessage] = {

    lines.map(line => {

      val arr = line.concat(" ").split("/f")

      ChannelMessage(channel = arr(0),
        channel_type = arr(1),
        msgType = arr(2),
        user = arr(3),
        event_time = arr(4),
        text = if (arr(5) == "") None else Some(arr(5)))
    }).repartition(2)
  }

  /**
  *   Подсчет частоты слов в минуту
   **/

  def decomposeMsg ( spark: SparkSession, sourceFilPath: String) /* : = RDD[WordsDistribution]*/ = {

    def toDT (dt : String) : String = {
      val dateString = dt.substring(4,8)  + "-" + dt.substring(2,4) + "-" + dt.substring(0, 2) + " " + dt.substring(9,17)
      dateString
    }

    val rdd: RDD[String] = spark.sparkContext.textFile(sourceFilPath)

    val splitRdd = rdd.map( line => line.split("/f") )

    val wordRdd = splitRdd.flatMap( arr => {
      val channel = arr(0)
      val channel_type = arr(1)
      val msgType = arr(2)
      val user = arr(3)
      val event_time = arr(4)
      val text = (if (arr.lift(5).isDefined) arr(5)  else  "_emptyString_").split(" ")

      text.map(word => List(toDT(event_time), word.replaceAll("""[\p{Punct}&&[^.]]""", "") ))
    } )

    wordRdd.take(10).foreach(println)

   // val data = wordRdd.map(row)

    val data = wordRdd.map(line => {
      Row(
        line(0),
        line(1))
    } )


    val DFschema = StructType(
      List(
        StructField("event_time", StringType, true),
        StructField("word", StringType, true)
      )
    )

    val Slack_DF = spark.createDataFrame(data , DFschema).coalesce(1)
    Slack_DF.show(10)

    val viewName = s"messages_view"
    Slack_DF.createOrReplaceTempView(viewName)

    val query = Source.fromFile("src/main/resources/Scripts/WordOutliers.sql").mkString.replace("$viewName", viewName)
    println(query)

    val msg_stats = spark.sql(query)
    msg_stats.show(100)

    // create properties object
    /*
    val prop = new java.util.Properties
    prop.setProperty("user", "vskachkov")
    prop.setProperty("driver", "com.mysql.jdbc.Driver")

    val jdbcUrl : String = "jdbc:mysql://localhost:3306/vskachkov"

    msg_stats.write.mode("append").jdbc(jdbcUrl, "words_stat2", prop)
*/
  }

  def wordCount (word: String, rdd: RDD[ChannelMessage]) : Int = {
    rdd.aggregate(0)(
      (cnt, msg) => msg.mentionsWord(word) match { case true => cnt + 1 case _ => cnt + 0 }, (_ + _))
  }

  def msgCount (rdd: RDD[ChannelMessage]): RDD[((String, String), Int)] = {
    rdd.map(msg => (( dateConverter(msg.event_time), msg.channel ), 1)).reduceByKey(_ + _).sortBy(- _._2)
  }

  def wordsCount (rdd: RDD[ChannelMessage], limit: Int = 1) : RDD[(String, Int)] = {
    val words = rdd.flatMap(x => x.text.get.split(' '))
        .map(x => (x, 1))
        .reduceByKey(_ + _)
        .filter(_._2 >= limit)
        .sortBy(-_._2)
    words
  }

  def dbExport () = {
    val jdbcUrl = "jdbc:mysql://localhost:3306/vskachkov"
    val connection = DriverManager.getConnection(jdbcUrl)
    connection.isClosed()
  }

  def dateConverter (d: String): String = {
    d.substring(4,8) + "-" + d.substring(2,4) + "-" + d.substring(0,2)
  }

  def dbWrite_RDD (rdd: RDD[((String, String), Int)]) = {

    val jdbcUrl : String = "jdbc:mysql://localhost:3306/vskachkov"

    rdd.foreach {
      x =>
        val conn = DriverManager.getConnection(jdbcUrl)

        val MessageDate = "'" + x._1._1.toString + "'"
        val Channel = "'" + x._1._2.toString + "'"
        val MessageCount =  x._2.toString

        val query = s"INSERT INTO messages_stat2 (MessageDate, Channel, MessageCount) VALUES ($MessageDate, $Channel, $MessageCount) "
        println(query)

        val del = conn.prepareStatement ("INSERT INTO messages_stat2 (MessageDate, Channel, MessageCount) VALUES (?,?,?) ")

        del.setString(1, x._1._1.toString)
        del.setString(2, x._1._2.toString)
        del.setString(3, x._2.toString)




        del.executeUpdate
      }

  }
  // ------------------------------
  // version 1 with SparkSession
  // ------------------------------


  def ChannelMessageDF(spark: SparkSession, sourceFilPath: String) = {
    import spark.implicits._

    val sourceFile: RDD[String] = spark.sparkContext.textFile(sourceFilPath)

    val rdd = sourceFile.map(line => {

      val arr = line.concat(" ").split("/f")

      ChannelMessage(channel = arr(0),
        channel_type = arr(1),
        msgType = arr(2),
        user = arr(3),
        event_time = arr(4),
        text = if (arr(5) == "") None else Some(arr(5)))
    })

    val Slack_DF = rdd.toDS()
    Slack_DF.show(10)

    val viewName = s"summed"
    Slack_DF.createOrReplaceTempView(viewName)
    val msg_stats = spark.sql(s"SELECT SUBSTRING(event_time, 1, 8) as MessageDate, channel as Channel, count(*) as MessageCount FROM $viewName GROUP BY SUBSTRING(event_time, 1, 8), channel ").coalesce(1)
    msg_stats.show()

    // create properties object
    val prop = new java.util.Properties
    prop.setProperty("driver", "com.mysql.jdbc.Driver")
    prop.setProperty("user", "vskachkov")

    val jdbcUrl : String = "jdbc:mysql://localhost:3306/vskachkov"

    msg_stats.write.mode("append").jdbc(jdbcUrl, "messages_stat3", prop)

   }

  // ------------------------------
  // version 2 with SparkContext
  // ------------------------------

  def ChannelMessageDF_2 (spark: SparkContext, rdd: RDD[String]) = {

    val sqlContext = new org.apache.spark.sql.SQLContext(spark)

    val data = rdd.map(_.split("/f").to[List]).map(row).repartition(2)

    val DFschema = StructType(
      List(
        StructField("channel", StringType, true),
        StructField("channel_type", StringType, true),
        StructField("msgType", StringType, true),
        StructField("user", StringType, true),
        StructField("event_time", StringType, true),
        StructField("text", StringType, true)
      )
    )

    val Slack_DF = sqlContext.createDataFrame(data , DFschema).repartition(2)

    Slack_DF.show(10)

    val viewName = s"summed2"
    Slack_DF.registerTempTable(viewName)  // for 2.0 createOrReplaceTempView(viewName)

    val msg_stats = sqlContext.sql(s"SELECT SUBSTRING(event_time, 1, 8) as MessageDate, channel as Channel, count(*) as MessageCount FROM $viewName GROUP BY SUBSTRING(event_time, 1, 8), channel ").repartition(2)
    msg_stats.show()

    // create properties object
    val prop = new java.util.Properties
    prop.setProperty("driver", "com.mysql.jdbc.Driver")
    prop.setProperty("user", "vskachkov")

    val jdbcUrl : String = "jdbc:mysql://localhost:3306/vskachkov"

    msg_stats.write.mode("append").jdbc(jdbcUrl, "messages_stat3", prop)

  }


  def row(line: List[String]): Row = {
      val header = line.head.toString
      val valueCols =line.tail.map {x => {if (x != null) x else 0D}
      }

      val par5 = line.lift(5).isDefined
    //    println(par5)
      val text =  if (par5) valueCols  else valueCols :+ "_emptyString_"
    //    println(header :: text)
      Row.fromSeq(header :: text )
    }

  def ChannelMessageDF_JSON (spark: SparkSession, path: String ) = {

    val DF_json = spark.read.json(path).coalesce(2)

    DF_json.printSchema()

    //val DF_agg = DF_json.select("event.channel").groupBy("event.channel").agg(count("event_id") as "event_count")
    //DF_agg.show()
    val viewName = s"DF_json"
    DF_json.createOrReplaceTempView(viewName)

    spark.sqlContext.sql("SELECT DISTINCT event.channel FROM DF_json").show
  }
}
